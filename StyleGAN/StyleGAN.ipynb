{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN\n",
    "\n",
    "Generative adversarial networks (GANs) have been around since [2014](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), and have shown steady progress in developing models that can generate realistic images. GANs work by creating two competing networks - a Generator and a Discriminator. The Generator maps a latent vector to an image. The Discriminator evaluates the quality of the generated image relative to some existing dataset. Over the training process, the generated images converge from random noise to images that resemble the target dataset.\n",
    "\n",
    "GANs ideally learn to generate images that *look like* the target dataset without actually *copying* the target dataset. GANs learn some probability distribution over the target dataset and generate images that look like the *could have* come from that dataset, without actually memorizing and regurgitating images from the dataset. In a perfect world, each different latent vector maps to a different output, allowing for infinitely many generated images. Latent inputs that are close in vector space generate images that are similar but not exactly the same. In practice however, GANs can suffer from what is known as Mode Collapse, where the Generator converges to only generating a small, finite set of outputs regardless of the input vector.\n",
    "\n",
    "GANs also suffer from stability problems and are notoriously hard to train. Historically, generating larger images while maintaining the quality and diversity possible at lower scales has proven challenging. Various techniques like [Progressive Growing](https://arxiv.org/abs/1710.10196), careful [weight clipping](https://arxiv.org/abs/1701.07875), [regularization strategies](https://arxiv.org/abs/1704.00028) and [normalization approaches](https://arxiv.org/abs/1802.05957) have been developed to improve the stability of training GANs.\n",
    "\n",
    "Another outstanding mystery in GAN-world is understanding how the latent vector maps to the output. Specifically, what parts of the latent input control aspects of the latent output. How can we break apart the latent vector and disentangle the latent representation to have finer control over the resulting image?\n",
    "\n",
    "In 2018, Nvidia released [StyleGAN](https://arxiv.org/pdf/1812.04948.pdf), a new type of GAN architecture that builds on the history of GAN research to create a fundamentally different kind of generator that tackles the problem of understanding the properties of the latent vector.\n",
    "\n",
    "Instead of starting with a latent vector, StyleGAN first sends the latent vector through a mapping network consisting of several fully connected layers. This allows the generator to learn an intermediate latent space that is best for generating images. The intermediate latent vector is then injected into the generator at different layers, rather than just being a starting template.\n",
    "\n",
    "![](media/stylegan.png)\n",
    "\n",
    "The key finding of StyleGAN was that adding the intermediate latent vector at different layers of the generator caused different effects on the output image. Injection at early layers of the generator influenced major details of the image, while injection at later layers influenced finer details. The result is that the StyleGAN generator can be used to blend different latent vectors. The exact method by which the intermediate latent vector is added to the generator is discussed later.\n",
    "\n",
    "![](media/stylegan_mixing.jpg)\n",
    "\n",
    "StyleGAN is able to train with good stability to images as large as 1024x1024 by training with [progressive growing](https://arxiv.org/pdf/1710.10196.pdf). We start training on 4x4 images. After some time, we grow to 8x8, then 16x16 and so on. When we grow the model, there's a small issue. We now have a new layer that converts a tensor of activations to a 3 channel RGB image. Since this new layer is untrained, it likely produces poor quality images. To prevent this from messing up training, the new RGB layer is phased in.\n",
    "\n",
    "![](media/rgb.png)\n",
    "\n",
    "So that's a quick overview. Letâ€™s start looking at the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.vision.gan import *\n",
    "from fastai.callbacks import *\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('G:/FFHQ/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(bs, size, path, num_workers, noise_size=512):\n",
    "    return (GANItemList.from_folder(path, noise_sz=noise_size)\n",
    "               .split_none()\n",
    "               .label_from_func(noop)\n",
    "               .transform(tfms=[[crop_pad(size=size, row_pct=(0,1), col_pct=(0,1))], []], size=size, tfm_y=True)\n",
    "               .databunch(bs=bs, num_workers=num_workers)\n",
    "               .normalize(stats = [torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5])], do_x=False, do_y=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I resized the main dataset into individual sets for each image size\n",
    "# This has a noticeable effect on training speed\n",
    "data_4 = get_data(256, 4, path/'images_4', 8)\n",
    "data_8 = get_data(256, 8, path/'images_8', 8)\n",
    "data_16 = get_data(128, 16, path/'images_16', 8)\n",
    "data_32 = get_data(128, 32, path/'images_32', 8)\n",
    "data_64 = get_data(48, 64, path/'images_64', 8)\n",
    "data_128 = get_data(22, 128, path/'images_128', 8)\n",
    "data_256 = get_data(10, 256, path/'images_256', 8)\n",
    "data_512 = get_data(4, 512, path/'images_512', 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StyleGAN Fundamentals\n",
    "\n",
    "First we start with the basic building blocks of the StyleGAN. StyleGAN uses Equalized Learning Rates for all layers. This is a technique from the [Progressive GAN](https://arxiv.org/pdf/1710.10196.pdf) paper.\n",
    "\n",
    "Layers are typically initialized to a careful distribution to ensure the initial output of the untrained layer has approximately mean zero and standard deviation one. This is to avoid having activations or gradients vanish or explode early in training. Kaiming initialization scales all values by a factor of $c = \\frac{\\sqrt2}{W}$ where $W$ is the length of the weight matrix along one dimension.\n",
    "\n",
    "Equalized learning rates applies the Kaiming constant dynamically during runtime, rather than just at the initialization. The reason for doing this is, to quote the paper, \"somewhat subtle\". Many optimization algorithms normalize gradients in such a way that the scale of the gradient update independent of the scale of the parameter being updated. This leads to some small parameters receiving updates that are too large, and large parameters receiving updates that are too small. The effect is that the learning rate is too high for some parameters and too low for others, leading to instability. The Equalized Learning Weights technique applies Kaiming scaling on every forward pass to ensure all the weights in the model are of a similar range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualLR:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def compute_weight(self, module):\n",
    "        weight = getattr(module, self.name + '_orig')\n",
    "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "\n",
    "        return weight * math.sqrt(2 / fan_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(module, name):\n",
    "        fn = EqualLR(name)\n",
    "\n",
    "        weight = getattr(module, name)\n",
    "        del module._parameters[name]\n",
    "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "        module.register_forward_pre_hook(fn)\n",
    "\n",
    "        return fn\n",
    "\n",
    "    def __call__(self, module, input):\n",
    "        weight = self.compute_weight(module)\n",
    "        setattr(module, self.name, weight)\n",
    "\n",
    "\n",
    "def equal_lr(module, name='weight'):\n",
    "    EqualLR.apply(module, name)\n",
    "\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualConv2d(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        conv = nn.Conv2d(*args, **kwargs)\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "        self.conv = equal_lr(conv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        linear = nn.Linear(in_dim, out_dim)\n",
    "        linear.weight.data.normal_()\n",
    "        linear.bias.data.zero_()\n",
    "\n",
    "        self.linear = equal_lr(linear)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard convolutional block for the model. It consists of two convolutional layers with leaky relu activations. This sort of convolutional block will be used in the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        kernel_size,\n",
    "        padding,\n",
    "        kernel_size2=None,\n",
    "        padding2=None,\n",
    "        pixel_norm=True,\n",
    "        spectral_norm=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        pad1 = padding\n",
    "        pad2 = padding\n",
    "        if padding2 is not None:\n",
    "            pad2 = padding2\n",
    "\n",
    "        kernel1 = kernel_size\n",
    "        kernel2 = kernel_size\n",
    "        if kernel_size2 is not None:\n",
    "            kernel2 = kernel_size2\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            EqualConv2d(in_channel, out_channel, kernel1, padding=pad1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualConv2d(out_channel, out_channel, kernel2, padding=pad2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv(input)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the generator, we alter the convolutional block with instance normalization. This is where the StyleGAN gets its style. Rather than normalizing activations by Batch Normalization, StyleGAN uses normalization with a learned affine transformation generated from the latent vector.\n",
    "\n",
    "First the input activations are normalized by the mean and standard deviation of the activations. Then the input vector, mapped to latent space by a series of fully connected layers, is put through a linear layer that outputs two vectors with length corresponding to the number of channels in the activation tensor being normalized. These output vectors are learned affine transformations of the activations.\n",
    "\n",
    "The full normalization is given by:\n",
    "\n",
    "$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "Where $\\gamma$ and $\\beta$ are the parameters generated from the latent vector.\n",
    "\n",
    "In a nutshell, StyleGAN works by putting the intermediate latent vector through a linear layer that generates parameters used to shift and scale activations at each layer of the network. Each layer has its own linear layer to process the intermediate latent vector into scaling parameters, so each layer can learn to use the intermediate latent vector in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNorm(nn.Module):\n",
    "    def __init__(self, in_channel, style_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.InstanceNorm2d(in_channel)\n",
    "        self.style = EqualLinear(style_dim, in_channel * 2)\n",
    "\n",
    "        self.style.linear.bias.data[:in_channel] = 1\n",
    "        self.style.linear.bias.data[in_channel:] = 0\n",
    "\n",
    "    def forward(self, input, style):\n",
    "        style = self.style(style).unsqueeze(2).unsqueeze(3)\n",
    "        gamma, beta = style.chunk(2, 1)\n",
    "\n",
    "        out = self.norm(input)\n",
    "        out = gamma * out + beta\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StyleGAN also injects noise at each convolutional layer. This noise gives variation to the fine details of the output image. In the StyleGAN paper this manifested as adding variety to things like specific locks of hair or facial wrinkles.\n",
    "\n",
    "The noise itself is drawn from a random normal distribution. The noise vector is then multiplied by a learned weight and added to the activations at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n",
    "\n",
    "    def forward(self, image, noise):\n",
    "        return image + self.weight * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StyleGAN replaces using a latent vector as input to injecting the latent vector at each layer of the model. So what does the model use as the initial input for the generator?\n",
    "\n",
    "StyleGAN uses a constant input that is learned over the course of training. This is really just a chunk of numbers to get the model started. All the variation in style comes from the latent vector injected throughout the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantInput(nn.Module):\n",
    "    def __init__(self, channel, size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Parameter(torch.randn(1, channel, size, size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch = input.shape[0]\n",
    "        out = self.input.repeat(batch, 1, 1, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basic convolutional block for the generator. The layer consists of two `Conv, Noise Injection, AdaIN, Leaky Relu` blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyledConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        kernel_size=3,\n",
    "        padding=1,\n",
    "        style_dim=512,\n",
    "        initial=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if initial:\n",
    "            self.conv1 = ConstantInput(in_channel)\n",
    "\n",
    "        else:\n",
    "            self.conv1 = EqualConv2d(\n",
    "                in_channel, out_channel, kernel_size, padding=padding\n",
    "            )\n",
    "\n",
    "        self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
    "        self.adain1 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
    "        self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
    "        self.adain2 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, input, style, noise):\n",
    "        out = self.conv1(input)\n",
    "        out = self.noise1(out, noise)\n",
    "        out = self.adain1(out, style)\n",
    "        out = self.lrelu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.noise2(out, noise)\n",
    "        out = self.adain2(out, style)\n",
    "        out = self.lrelu2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the styled conv block, we can create the generator for our network. The core of the network is the `progression` block, which upscales the constant input to a size of up to 16x1024x1024. The `to_rgb` stack contains EqualConv layers which pair with every layer in the `progression` block to convert the activations from `progression` into 3 channel RGB images.\n",
    "\n",
    "The model takes as main inputs some style, which will be created by the mapping network, and some random noise. The step parameter controls how far through the `progression` stack we move before exiting to the `to_rgb` stack. The alpha parameter controls blending between a given `to_rgb` layer and the previous `to_rgb` layer.\n",
    "\n",
    "The forward pass supports style mixing regularization. From the StyleGAN paper, this was shown to train the model to use the style injected at a given layer, and not rely on style inputs from previous layers. This made the effect of style mixing at different layers more dramatic. A `mixing_range` can be passed to control when styles are mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, code_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.progression = nn.ModuleList(\n",
    "            [\n",
    "                StyledConvBlock(512, 512, 3, 1, initial=True), # 4x4\n",
    "                StyledConvBlock(512, 512, 3, 1), # 8x8\n",
    "                StyledConvBlock(512, 512, 3, 1), # 16x16\n",
    "                StyledConvBlock(512, 512, 3, 1), # 32x32\n",
    "                StyledConvBlock(512, 256, 3, 1), # 64x64\n",
    "                StyledConvBlock(256, 128, 3, 1), # 128x128\n",
    "                StyledConvBlock(128, 64, 3, 1), # 256x256\n",
    "                StyledConvBlock(64, 32, 3, 1), # 512x512\n",
    "                StyledConvBlock(32, 16, 3, 1), # 1024x1024\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.to_rgb = nn.ModuleList(\n",
    "            [\n",
    "                EqualConv2d(512, 3, 1), # 4x4\n",
    "                EqualConv2d(512, 3, 1), # 8x8\n",
    "                EqualConv2d(512, 3, 1), # 16x16\n",
    "                EqualConv2d(512, 3, 1), # 32x32\n",
    "                EqualConv2d(256, 3, 1), # 64x64\n",
    "                EqualConv2d(128, 3, 1), # 128x128\n",
    "                EqualConv2d(64, 3, 1), # 256x256\n",
    "                EqualConv2d(32, 3, 1), # 512x512\n",
    "                EqualConv2d(16, 3, 1), # 1024x1024\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, style, noise, step=0, alpha=-1, mixing_range=(-1, -1)):\n",
    "        out = noise[0]\n",
    "\n",
    "        if len(style) < 2:\n",
    "            inject_index = [len(self.progression) + 1]\n",
    "\n",
    "        else:\n",
    "            inject_index = random.sample(list(range(step)), len(style) - 1)\n",
    "\n",
    "        crossover = 0\n",
    "\n",
    "        for i, (conv, to_rgb) in enumerate(zip(self.progression, self.to_rgb)):\n",
    "            if mixing_range == (-1, -1):\n",
    "                if crossover < len(inject_index) and i > inject_index[crossover]:\n",
    "                    crossover = min(crossover + 1, len(style))\n",
    "\n",
    "                style_step = style[crossover]\n",
    "\n",
    "            else:\n",
    "                if mixing_range[0] <= i <= mixing_range[1]:\n",
    "                    style_step = style[1]\n",
    "\n",
    "                else:\n",
    "                    style_step = style[0]\n",
    "\n",
    "            if i > 0 and step > 0:\n",
    "                upsample = F.interpolate(\n",
    "                    out, scale_factor=2, mode='bilinear', align_corners=False\n",
    "                )\n",
    "                out = conv(upsample, style_step, noise[i])\n",
    "\n",
    "            else:\n",
    "                out = conv(out, style_step, noise[i])\n",
    "\n",
    "            if i == step:\n",
    "                out = to_rgb(out)\n",
    "\n",
    "                if i > 0 and alpha > 0:\n",
    "                    skip_rgb = self.to_rgb[i - 1](upsample)\n",
    "                    out = alpha * skip_rgb + (1 - alpha) * out\n",
    "\n",
    "                break\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the entire generator network. The `StyledGenerator` class contains the linear mapping network that creates style vectors from random vectors. We also include a PixelNorm layer at the start to normalize the random vector. The full generator also stores the current `step` of the model and `alpha` for RGB mixing as attributes.\n",
    "\n",
    "The forward pass generates style vectors from the inputs and includes possible random shuffling of style vectors for mixing regularization. Noise can be explicitly passed or randomly generated during the forward pass.\n",
    "\n",
    "A `mean_style` can also be passed to enable truncation via the `style_weight` parameter.\n",
    "\n",
    "The full generator has a `grow_model` function which can be used to dynamically control the size of the image generated and the degree of RGB mixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyledGenerator(nn.Module):\n",
    "    def __init__(self, code_dim=512, n_mlp=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = Generator(code_dim)\n",
    "\n",
    "        layers = [PixelNorm()]\n",
    "        for i in range(n_mlp):\n",
    "            layers.append(EqualLinear(code_dim, code_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.style = nn.Sequential(*layers)\n",
    "        \n",
    "        self.step = 0\n",
    "        self.alpha = 0\n",
    "\n",
    "    def forward(self, input, noise=None, mean_style=None, style_weight=0, mixing_range=(-1, -1), mixing=True):\n",
    "        \n",
    "        bs, ch, _, _ = input.shape\n",
    "        input = input.view(bs, ch)\n",
    "        \n",
    "        if self.step < 1:\n",
    "            mixing=False\n",
    "        \n",
    "        if mixing and random.random() < 0.9:\n",
    "            shuffle = torch.randperm(input.size(0)).to(input.device)\n",
    "            input = [input, input[shuffle]]\n",
    "        else:\n",
    "            input = [input]  \n",
    "        \n",
    "        styles = []\n",
    "\n",
    "        for i in input:\n",
    "            styles.append(self.style(i))\n",
    "\n",
    "        batch = input[0].shape[0]\n",
    "\n",
    "        if noise is None:\n",
    "            noise = []\n",
    "\n",
    "            for i in range(self.step + 1):\n",
    "                size = 4 * 2 ** i\n",
    "                noise.append(torch.randn(batch, 1, size, size, device=input[0].device))\n",
    "\n",
    "        if mean_style is not None:\n",
    "            styles_norm = []\n",
    "\n",
    "            for style in styles:\n",
    "                styles_norm.append(mean_style + style_weight * (style - mean_style))\n",
    "\n",
    "            styles = styles_norm\n",
    "\n",
    "        return self.generator(styles, noise, self.step, self.alpha, mixing_range=mixing_range)\n",
    "\n",
    "    def mean_style(self, input):\n",
    "        style = self.style(input).mean(0, keepdim=True)\n",
    "\n",
    "        return style\n",
    "        \n",
    "    def grow_model(self, step=None, alpha=None):\n",
    "        if step:\n",
    "            self.step = step\n",
    "        else:\n",
    "            self.step += 1\n",
    "            \n",
    "        if alpha is not None:\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Discriminator mirrors the generator. EqualConv blocks take in a 3 channel RGB image and project it to some higher channel tensor. Then a `progression` stack convolves the tensor down. The same step and alpha logic applies.\n",
    "\n",
    "There's also a funny `actual` parameter in the forward loop. When `actual` is set to False, the discriminator returns a tuple of `(None, input)`. This is to help the discriminator and the gradient regularization we will apply better plug into the fast.ai framework. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.progression = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(16, 32, 3, 1),\n",
    "                ConvBlock(32, 64, 3, 1),\n",
    "                ConvBlock(64, 128, 3, 1),\n",
    "                ConvBlock(128, 256, 3, 1),\n",
    "                ConvBlock(256, 512, 3, 1),\n",
    "                ConvBlock(512, 512, 3, 1),\n",
    "                ConvBlock(512, 512, 3, 1),\n",
    "                ConvBlock(512, 512, 3, 1),\n",
    "                ConvBlock(513, 512, 3, 1, 4, 0),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.from_rgb = nn.ModuleList(\n",
    "            [\n",
    "                EqualConv2d(3, 16, 1),\n",
    "                EqualConv2d(3, 32, 1),\n",
    "                EqualConv2d(3, 64, 1),\n",
    "                EqualConv2d(3, 128, 1),\n",
    "                EqualConv2d(3, 256, 1),\n",
    "                EqualConv2d(3, 512, 1),\n",
    "                EqualConv2d(3, 512, 1),\n",
    "                EqualConv2d(3, 512, 1),\n",
    "                EqualConv2d(3, 512, 1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.n_layer = len(self.progression)\n",
    "\n",
    "        self.linear = EqualLinear(512, 1)\n",
    "        \n",
    "        self.step = 0\n",
    "        self.alpha = 0\n",
    "\n",
    "    def forward(self, input, actual=False):\n",
    "        if actual:\n",
    "            for i in range(self.step, -1, -1):\n",
    "                index = self.n_layer - i - 1\n",
    "\n",
    "                if i == self.step:\n",
    "                    out = self.from_rgb[index](input)\n",
    "\n",
    "                if i == 0:\n",
    "                    out_std = torch.sqrt(out.var(0, unbiased=False) + 1e-8)\n",
    "                    mean_std = out_std.mean()\n",
    "                    mean_std = mean_std.expand(out.size(0), 1, 4, 4)\n",
    "                    out = torch.cat([out, mean_std], 1)\n",
    "\n",
    "                out = self.progression[index](out)\n",
    "\n",
    "                if i > 0:\n",
    "                    out = F.interpolate(\n",
    "                        out, scale_factor=0.5, mode='bilinear', align_corners=False\n",
    "                    )\n",
    "\n",
    "                    if i == self.step and self.alpha > 0:  #0 <= alpha < 1:\n",
    "                        skip_rgb = self.from_rgb[index + 1](input)\n",
    "                        skip_rgb = F.interpolate(\n",
    "                            skip_rgb, scale_factor=0.5, mode='bilinear', align_corners=False\n",
    "                        )\n",
    "\n",
    "                        out = self.alpha * skip_rgb + (1 - self.alpha) * out\n",
    "\n",
    "            out = out.squeeze(2).squeeze(2)\n",
    "            out = self.linear(out)\n",
    "\n",
    "            return (out, input)\n",
    "        else:\n",
    "            return (None, input)\n",
    "    \n",
    "    def grow_model(self, step=None, alpha=None):\n",
    "        if step:\n",
    "            self.step = step\n",
    "        else:\n",
    "            self.step += 1\n",
    "            \n",
    "        if alpha is not None:\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Calculation\n",
    "\n",
    "With our models in hand, we can move to the loss function. StyleGAN utilizes the non-saturating softplus loss with R1 gradient penalty from [Which Training Methods for GANs do actually Converge?](https://arxiv.org/abs/1801.04406).\n",
    "\n",
    "The R1 penalty adds a loss term for the magnitude of the gradient of the discriminator with respect to a real input. This entails calculating the gradient of the discriminator's prediction on real data with respect to the real data. This is different from the typical paradigm of calculating loss with respect to the weights.\n",
    "\n",
    "The R1 gradient penalty is added only for the discriminator loss.\n",
    "\n",
    "The code below shows the loss calculations for the generator and the discriminator. The loss calculation for the discriminator takes in as input a prediction on fake data and a prediction on real data that allows for calculation of the gradient with respect to the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleCriticLoss_R1(nn.Module):\n",
    "    \n",
    "    def forward(self, fake, gradient_predict):\n",
    "        real_predict = F.softplus(-gradient_predict[0]).mean()\n",
    "        \n",
    "        grad_real = torch.autograd.grad(outputs=gradient_predict[0].sum(), inputs=gradient_predict[1], create_graph=True)[0]\n",
    "        grad_penalty = grad_real.view(grad_real.size(0), -1).norm(2, dim=1).pow(2).mean()\n",
    "        grad_penalty = 10 / 2 * grad_penalty\n",
    "        \n",
    "        fake_predict = F.softplus(fake).mean()\n",
    "        \n",
    "        loss = real_predict + fake_predict + grad_penalty\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class StyleGenLoss_R1(nn.Module):\n",
    "    \n",
    "    def forward(self, fake_pred, *args):\n",
    "        return F.softplus(-fake_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code handles the actual calculation of the loss for both models. It hooks into the `GANModule` class of the fast.ai library which manages switching between models during training. \n",
    "\n",
    "The generator calculation is straightforward. The discriminator makes a prediction on a fake image, and that value is fed to the generator loss function.\n",
    "\n",
    "For the critic, we have to do some funky stuff. Remember the discriminator has that weird `actual` argument in the forward pass that can be set to `False` to avoid running the actual forward pass, returning just the input instead. The code below shows why we do this.\n",
    "\n",
    "The `GANModule` class automatically runs the forward pass of the critic on real data, passing the result as the input to the `critic` function below, which feeds into the discriminator loss function. Without seriously altering the library, there was no way to implement the R1 gradient regularization, which requires setting the gradient flag on the real input before passing it to the discriminator. Adding the `actual` clause to skip the forward pass in the discriminator was a quick way around this.\n",
    "\n",
    "Instead, we do the forward pass of the discriminator in the `critic` function. First we calculate the discriminator output on fake, generated images. Then we get a batch of real data from the `real_pred` input. We enable gradient calculation on the real input, then pass it to the discriminator. The results of these calculations are passed to the discriminator loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLossGradient(GANModule):\n",
    "    \"Wrapper around `loss_funcC` (for the critic) and `loss_funcG` (for the generator).\"\n",
    "    def __init__(self, loss_funcG:Callable, loss_funcC:Callable, gan_model:GANModule, grad_scale=10):\n",
    "        super().__init__()\n",
    "        self.loss_funcG,self.loss_funcC,self.gan_model = loss_funcG,loss_funcC,gan_model\n",
    "        self.grad_scale = grad_scale\n",
    "\n",
    "    def generator(self, output, target):\n",
    "        \"Evaluate the `output` with the critic then uses `self.loss_funcG` to combine it with `target`.\"\n",
    "        fake_pred = self.gan_model.critic(output, actual=True)[0]\n",
    "        return self.loss_funcG(fake_pred, target, output)\n",
    "\n",
    "    def critic(self, real_pred, input):\n",
    "        \"Create some `fake_pred` with the generator from `input` and compare them to `real_pred` in `self.loss_funcD`.\"\n",
    "        fake = self.gan_model.generator(input.requires_grad_(False)).requires_grad_(True)\n",
    "        fake_pred = self.gan_model.critic(fake, actual=True)\n",
    "        \n",
    "        real_pred[1].requires_grad = True\n",
    "        gradient_predict = self.gan_model.critic(real_pred[1], actual=True)\n",
    "        \n",
    "        return self.loss_funcC(fake_pred[0], gradient_predict)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Implementation Details\n",
    "\n",
    "We also need to implement the fade in between layers described earlier. We do this using a callback function. At the start of every batch, this callback linearly decays the value of the `alpha` attribute in the generator and the discriminator over a set number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaDecay(LearnerCallback):\n",
    "    \n",
    "    def __init__(self, learn, end_batch):\n",
    "        super().__init__(learn)\n",
    "        self.end_batch = end_batch\n",
    "        self.initial_alpha = self.learn.model.generator.alpha\n",
    "        \n",
    "    def on_batch_begin(self, iteration, **kwargs):\n",
    "        if self.learn.model.generator.alpha > 0:\n",
    "            self.learn.model.generator.alpha = self.initial_alpha - iteration / self.end_batch\n",
    "            self.learn.model.critic.alpha = self.initial_alpha - iteration / self.end_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new `Learner` subclass to put everything together. This creates an object holding both the generator and the discriminator that handles switching between the discriminator and the generator during training, loss calculation with R1 penalty and decaying the `alpha` parameter.\n",
    "\n",
    "We also implement a `grow_model` function to control output size and alpha decay. `grow_model` takes in a new dataloader for images of a different size, a `step` value to control how far to move through the generator and discriminator models, an `alpha` parameter to set new `alpha` values for the generator and discriminator, and an `alpha_batch` parameter that controls the rate of alpha decay.\n",
    "\n",
    "Typically the length of the stabilization fade in period is measured by the number of images, not the number of batches. The StyleGAN paper fades for 800,000 images, then stabilizes for another 800,000 images before moving on to the next size up. Typically your batch size gets smaller as images get larger, so your `alpha_batch` value increases inversely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGANLearner(Learner):\n",
    "    \"A `Learner` suitable for GANs.\"\n",
    "    def __init__(self, data:DataBunch, generator:nn.Module, critic:nn.Module, gen_loss_func:LossFunction,\n",
    "                 crit_loss_func:LossFunction, switcher:Callback=None, gen_first:bool=False, switch_eval:bool=True,\n",
    "                 show_img:bool=True, clip:float=None, alpha_batch=10, **learn_kwargs):\n",
    "        gan = GANModule(generator, critic)\n",
    "        loss_func = GANLossGradient(gen_loss_func, crit_loss_func, gan)\n",
    "        switcher = ifnone(switcher, partial(FixedGANSwitcher, n_crit=5, n_gen=1))\n",
    "        super().__init__(data, gan, loss_func=loss_func, callback_fns=[switcher], **learn_kwargs)\n",
    "        trainer = GANTrainer(self, clip=clip, switch_eval=switch_eval, show_img=show_img)\n",
    "        self.gan_trainer = trainer\n",
    "        self.callbacks.append(trainer)\n",
    "        self.callback_fns.append(partial(AlphaDecay, end_batch=alpha_batch))\n",
    "    \n",
    "    @classmethod\n",
    "    def stylegan_r1(cls, data:DataBunch, generator:nn.Module, critic:nn.Module, \n",
    "             switcher:Callback=None, clip:float=None, **learn_kwargs):\n",
    "        \"Create a WGAN from `data`, `generator` and `critic`.\"\n",
    "        return cls(data, generator, critic, StyleGenLoss_R1(), \n",
    "                       StyleCriticLoss_R1(), switcher=switcher, clip=clip, **learn_kwargs)\n",
    "\n",
    "    def grow_model(self, data, step=None, alpha=None, alpha_batch=None):\n",
    "        self.model.generator.grow_model(step=step, alpha=alpha)\n",
    "        self.model.critic.grow_model(step=step, alpha=alpha)\n",
    "        self.data = data\n",
    "        \n",
    "        if alpha_batch is not None:\n",
    "            self.callback_fns[-1] = partial(AlphaDecay, end_batch=alpha_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now we step through training with progressive resizing. We start with 4x4 images and work up to 256x256 images. With the dataset, I could go as high as 512x512 images but I am limited by compute. Training for two epochs at 256x256 already took almost two days.\n",
    "\n",
    "For optimization we use the Adam optimizer with $\\beta_1$ set to 0 (momentum typically works poorly with adversarial models). We don't use weight decay or gradient clipping. We train the discriminator for 2 epochs for every 1 onf the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = StyledGenerator()\n",
    "d = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = StyleGANLearner.stylegan_r1(data_4, g, d, switch_eval=False,\n",
    "                        opt_func = partial(optim.Adam, betas = (0.,0.99)), wd=0., alpha_batch=6000, clip=None,\n",
    "                              switcher=partial(FixedGANSwitcher, n_crit=2, n_gen=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define learning rate groups. Following the StyleGAN paper, we training the mapping network of the generator at 0.01x the learning rate of the generator at higher resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_split(model):\n",
    "    groups = [[model.generator.style]]\n",
    "    groups += [[model.generator.generator, model.critic]]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.split(style_split);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cells show snapshots of training. Unfortunately the denormalization of images for this function in the fast.ai library isn't implemented properly, so the training snapshots look wrong. See examples at the end of training for proper denormalization of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
